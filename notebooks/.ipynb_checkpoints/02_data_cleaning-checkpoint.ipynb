{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procurement KPI Analytics - Data Cleaning\n",
    "\n",
    "**Objective**: Clean and standardize procurement data to ensure high quality for analysis and modeling.\n",
    "\n",
    "**Key Tasks**:\n",
    "- Handle missing values and data inconsistencies\n",
    "- Standardize date formats and categorical variables\n",
    "- Identify and handle outliers\n",
    "- Validate data integrity and business rules\n",
    "- Create clean datasets for downstream analysis\n",
    "\n",
    "**Input**: Raw procurement dataset\n",
    "**Output**: Clean, validated dataset ready for analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "# Optional imports with fallback\n",
    "try:\n",
    "    from scipy import stats\n",
    "    SCIPY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: scipy not available. Some outlier detection methods will be limited.\")\n",
    "    SCIPY_AVAILABLE = False\n",
    "\n",
    "# Configure display and warnings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "print(\"Libraries loaded successfully!\")\n",
    "print(f\"Analysis timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw dataset\n",
    "try:\n",
    "    df_raw = pd.read_csv('../data/raw/Procurement_KPI_Analysis_Dataset.csv')\n",
    "    print(\"Raw dataset loaded successfully!\")\n",
    "    print(f\"Original dataset shape: {df_raw.shape[0]:,} rows x {df_raw.shape[1]} columns\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Dataset not found. Please ensure the file exists in '../data/raw/'\")\n",
    "    print(\"Expected filename: 'Procurement_KPI_Analysis_Dataset.csv'\")\n",
    "\n",
    "# Create working copy\n",
    "df = df_raw.copy()\n",
    "print(f\"Working dataset created with {len(df):,} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial data overview\n",
    "print(\"Initial Data Overview:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(\"\\nColumn data types:\")\n",
    "display(df.dtypes.to_frame('Data_Type'))\n",
    "\n",
    "print(\"\\nFirst 3 rows:\")\n",
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive data quality assessment\n",
    "def assess_data_quality(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Comprehensive data quality assessment function.\n",
    "    \"\"\"\n",
    "    quality_report = pd.DataFrame({\n",
    "        'Column': df.columns,\n",
    "        'Data_Type': df.dtypes,\n",
    "        'Non_Null_Count': df.count(),\n",
    "        'Null_Count': df.isnull().sum(),\n",
    "        'Null_Percentage': (df.isnull().sum() / len(df) * 100).round(2),\n",
    "        'Unique_Count': df.nunique(),\n",
    "        'Duplicate_Percentage': ((df.count() - df.nunique()) / df.count() * 100).round(2)\n",
    "    })\n",
    "    \n",
    "    # Add additional quality metrics for numerical columns\n",
    "    quality_report['Has_Zeros'] = False\n",
    "    quality_report['Has_Negatives'] = False\n",
    "    \n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        quality_report.loc[quality_report['Column'] == col, 'Has_Zeros'] = (df[col] == 0).any()\n",
    "        quality_report.loc[quality_report['Column'] == col, 'Has_Negatives'] = (df[col] < 0).any()\n",
    "    \n",
    "    return quality_report\n",
    "\n",
    "# Generate quality report\n",
    "print(\"Data Quality Assessment:\")\n",
    "print(\"=\" * 50)\n",
    "quality_report = assess_data_quality(df)\n",
    "display(quality_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify critical data quality issues\n",
    "print(\"Critical Data Quality Issues:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "issues_found = []\n",
    "\n",
    "# Check for missing values\n",
    "missing_cols = quality_report[quality_report['Null_Count'] > 0]['Column'].tolist()\n",
    "if missing_cols:\n",
    "    issues_found.append(f\"Missing values in columns: {', '.join(missing_cols)}\")\n",
    "\n",
    "# Check for potential ID column issues\n",
    "if 'PO_ID' in df.columns:\n",
    "    duplicate_ids = df['PO_ID'].duplicated().sum()\n",
    "    if duplicate_ids > 0:\n",
    "        issues_found.append(f\"Duplicate PO_IDs found: {duplicate_ids} records\")\n",
    "\n",
    "# Check for negative values where they shouldn't exist\n",
    "value_columns = ['Quantity', 'Unit_Price', 'Negotiated_Price', 'Defective_Units']\n",
    "for col in value_columns:\n",
    "    if col in df.columns and (df[col] < 0).any():\n",
    "        neg_count = (df[col] < 0).sum()\n",
    "        issues_found.append(f\"Negative values in {col}: {neg_count} records\")\n",
    "\n",
    "# Check for unrealistic defective units (more than quantity)\n",
    "if 'Defective_Units' in df.columns and 'Quantity' in df.columns:\n",
    "    unrealistic_defects = (df['Defective_Units'] > df['Quantity']).sum()\n",
    "    if unrealistic_defects > 0:\n",
    "        issues_found.append(f\"Defective units > Quantity: {unrealistic_defects} records\")\n",
    "\n",
    "# Check for price inconsistencies\n",
    "if 'Unit_Price' in df.columns and 'Negotiated_Price' in df.columns:\n",
    "    price_increases = (df['Negotiated_Price'] > df['Unit_Price'] * 1.5).sum()\n",
    "    if price_increases > 0:\n",
    "        issues_found.append(f\"Negotiated price >150% of unit price: {price_increases} records\")\n",
    "\n",
    "if issues_found:\n",
    "    for i, issue in enumerate(issues_found, 1):\n",
    "        print(f\"{i}. {issue}\")\nelse:\n",
    "    print(\"No critical data quality issues detected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze missing value patterns\n",
    "print(\"Missing Value Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "missing_summary = df.isnull().sum()\n",
    "missing_summary = missing_summary[missing_summary > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_summary) > 0:\n",
    "    print(\"Columns with missing values:\")\n",
    "    for col, count in missing_summary.items():\n",
    "        percentage = (count / len(df) * 100)\n",
    "        print(f\"  {col}: {count:,} missing ({percentage:.2f}%)\")\n",
    "    \n",
    "    # Visualize missing value patterns\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_data = missing_data[missing_data > 0]\n",
    "    \n",
    "    if len(missing_data) > 0:\n",
    "        missing_data.plot(kind='bar')\n",
    "        plt.title('Missing Values by Column')\n",
    "        plt.xlabel('Columns')\n",
    "        plt.ylabel('Number of Missing Values')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\nelse:\n",
    "    print(\"No missing values detected in the dataset!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values based on business logic\n",
    "def handle_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Handle missing values using domain-specific business logic.\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Track changes\n",
    "    changes_log = []\n",
    "    \n",
    "    # Handle missing Defective_Units (assume 0 if missing)\n",
    "    if 'Defective_Units' in df_clean.columns and df_clean['Defective_Units'].isnull().any():\n",
    "        missing_count = df_clean['Defective_Units'].isnull().sum()\n",
    "        df_clean['Defective_Units'] = df_clean['Defective_Units'].fillna(0)\n",
    "        changes_log.append(f\"Filled {missing_count} missing Defective_Units with 0\")\n",
    "    \n",
    "    # Handle missing Compliance (investigate patterns)\n",
    "    if 'Compliance' in df_clean.columns and df_clean['Compliance'].isnull().any():\n",
    "        missing_count = df_clean['Compliance'].isnull().sum()\n",
    "        # For now, mark as 'Unknown' - may need domain expert input\n",
    "        df_clean['Compliance'] = df_clean['Compliance'].fillna('Unknown')\n",
    "        changes_log.append(f\"Filled {missing_count} missing Compliance values with 'Unknown'\")\n",
    "    \n",
    "    # Handle missing dates (would need business rules)\n",
    "    date_columns = ['Order_Date', 'Delivery_Date']\n",
    "    for col in date_columns:\n",
    "        if col in df_clean.columns and df_clean[col].isnull().any():\n",
    "            missing_count = df_clean[col].isnull().sum()\n",
    "            changes_log.append(f\"Found {missing_count} missing values in {col} - requires business decision\")\n",
    "    \n",
    "    # Log all changes\n",
    "    if changes_log:\n",
    "        print(\"Missing value handling actions:\")\n",
    "        for change in changes_log:\n",
    "            print(f\"  - {change}\")\n",
    "    else:\n",
    "        print(\"No missing value handling required.\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Apply missing value handling\n",
    "print(\"Handling Missing Values:\")\n",
    "print(\"=\" * 50)\n",
    "df_cleaned = handle_missing_values(df)\n",
    "\n",
    "# Verify missing value handling\n",
    "remaining_missing = df_cleaned.isnull().sum().sum()\n",
    "print(f\"\\nRemaining missing values after cleaning: {remaining_missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Standardize Data Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize categorical variables\n",
    "def standardize_categorical_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Standardize categorical variables for consistency.\n",
    "    \"\"\"\n",
    "    df_std = df.copy()\n",
    "    \n",
    "    # Standardize text columns\n",
    "    text_columns = ['Supplier', 'Item_Category', 'Order_Status', 'Compliance']\n",
    "    \n",
    "    for col in text_columns:\n",
    "        if col in df_std.columns:\n",
    "            # Remove leading/trailing whitespace and standardize case\n",
    "            df_std[col] = df_std[col].astype(str).str.strip()\n",
    "            \n",
    "            # Show unique values before standardization\n",
    "            print(f\"\\n{col} - Unique values before standardization: {df_std[col].nunique()}\")\n",
    "            if df_std[col].nunique() <= 20:\n",
    "                print(f\"Values: {sorted(df_std[col].unique())}\")\n",
    "            \n",
    "            # Standardize specific columns\n",
    "            if col == 'Order_Status':\n",
    "                # Standardize order status values\n",
    "                status_mapping = {\n",
    "                    'completed': 'Completed',\n",
    "                    'COMPLETED': 'Completed',\n",
    "                    'complete': 'Completed',\n",
    "                    'pending': 'Pending',\n",
    "                    'PENDING': 'Pending',\n",
    "                    'cancelled': 'Cancelled',\n",
    "                    'CANCELLED': 'Cancelled',\n",
    "                    'canceled': 'Cancelled'\n",
    "                }\n",
    "                df_std[col] = df_std[col].replace(status_mapping)\n",
    "            \n",
    "            elif col == 'Compliance':\n",
    "                # Standardize compliance values\n",
    "                compliance_mapping = {\n",
    "                    'yes': 'Compliant',\n",
    "                    'YES': 'Compliant',\n",
    "                    'y': 'Compliant',\n",
    "                    'compliant': 'Compliant',\n",
    "                    'COMPLIANT': 'Compliant',\n",
    "                    'no': 'Non-Compliant',\n",
    "                    'NO': 'Non-Compliant',\n",
    "                    'n': 'Non-Compliant',\n",
    "                    'non-compliant': 'Non-Compliant',\n",
    "                    'NON-COMPLIANT': 'Non-Compliant',\n",
    "                    'noncompliant': 'Non-Compliant'\n",
    "                }\n",
    "                df_std[col] = df_std[col].replace(compliance_mapping)\n",
    "            \n",
    "            # Show unique values after standardization\n",
    "            print(f\"{col} - Unique values after standardization: {df_std[col].nunique()}\")\n",
    "            if df_std[col].nunique() <= 20:\n",
    "                print(f\"Values: {sorted(df_std[col].unique())}\")\n",
    "    \n",
    "    return df_std\n",
    "\n",
    "print(\"Standardizing Categorical Data:\")\n",
    "print(\"=\" * 50)\n",
    "df_cleaned = standardize_categorical_data(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize date formats\n",
    "def standardize_dates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert and standardize date columns.\n",
    "    \"\"\"\n",
    "    df_dates = df.copy()\n",
    "    \n",
    "    date_columns = ['Order_Date', 'Delivery_Date']\n",
    "    \n",
    "    for col in date_columns:\n",
    "        if col in df_dates.columns:\n",
    "            print(f\"\\nProcessing {col}:\")\n",
    "            print(f\"  Sample values: {df_dates[col].head(3).tolist()}\")\n",
    "            \n",
    "            try:\n",
    "                # Convert to datetime\n",
    "                df_dates[col] = pd.to_datetime(df_dates[col], errors='coerce')\n",
    "                \n",
    "                # Check for invalid dates\n",
    "                invalid_dates = df_dates[col].isnull().sum()\n",
    "                if invalid_dates > 0:\n",
    "                    print(f\"  Warning: {invalid_dates} invalid dates found\")\n",
    "                \n",
    "                # Show date range\n",
    "                if not df_dates[col].isnull().all():\n",
    "                    print(f\"  Date range: {df_dates[col].min()} to {df_dates[col].max()}\")\n",
    "                    print(f\"  Total days span: {(df_dates[col].max() - df_dates[col].min()).days} days\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Error converting {col}: {e}\")\n",
    "    \n",
    "    return df_dates\n",
    "\n",
    "print(\"Standardizing Date Formats:\")\n",
    "print(\"=\" * 50)\n",
    "df_cleaned = standardize_dates(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Validation and Business Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement business rule validations\n",
    "def validate_business_rules(df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    Validate data against business rules and flag violations.\n",
    "    \"\"\"\n",
    "    df_validated = df.copy()\n",
    "    violations = []\n",
    "    \n",
    "    print(\"Business Rule Validation:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Rule 1: Quantity must be positive\n",
    "    if 'Quantity' in df_validated.columns:\n",
    "        invalid_qty = (df_validated['Quantity'] <= 0).sum()\n",
    "        if invalid_qty > 0:\n",
    "            violations.append(f\"Rule 1 Violation: {invalid_qty} records with non-positive quantity\")\n",
    "            print(f\"  Rule 1: Found {invalid_qty} records with invalid quantity\")\n",
    "        else:\n",
    "            print(\"  Rule 1: All quantities are positive - PASS\")\n",
    "    \n",
    "    # Rule 2: Defective units cannot exceed total quantity\n",
    "    if 'Defective_Units' in df_validated.columns and 'Quantity' in df_validated.columns:\n",
    "        excess_defects = (df_validated['Defective_Units'] > df_validated['Quantity']).sum()\n",
    "        if excess_defects > 0:\n",
    "            violations.append(f\"Rule 2 Violation: {excess_defects} records with defective units > quantity\")\n",
    "            print(f\"  Rule 2: Found {excess_defects} records with excess defective units\")\n",
    "            # Flag these records\n",
    "            df_validated['excess_defects_flag'] = df_validated['Defective_Units'] > df_validated['Quantity']\n",
    "        else:\n",
    "            print(\"  Rule 2: All defective units within valid range - PASS\")\n",
    "    \n",
    "    # Rule 3: Delivery date should be after or equal to order date\n",
    "    if 'Order_Date' in df_validated.columns and 'Delivery_Date' in df_validated.columns:\n",
    "        # Only check where both dates are present\n",
    "        both_dates_present = df_validated['Order_Date'].notna() & df_validated['Delivery_Date'].notna()\n",
    "        invalid_dates = (df_validated.loc[both_dates_present, 'Delivery_Date'] < \n",
    "                        df_validated.loc[both_dates_present, 'Order_Date']).sum()\n",
    "        \n",
    "        if invalid_dates > 0:\n",
    "            violations.append(f\"Rule 3 Violation: {invalid_dates} records with delivery date before order date\")\n",
    "            print(f\"  Rule 3: Found {invalid_dates} records with invalid date sequence\")\n",
    "        else:\n",
    "            print(\"  Rule 3: All delivery dates are valid - PASS\")\n",
    "    \n",
    "    # Rule 4: Prices must be positive\n",
    "    price_columns = ['Unit_Price', 'Negotiated_Price']\n",
    "    for col in price_columns:\n",
    "        if col in df_validated.columns:\n",
    "            negative_prices = (df_validated[col] <= 0).sum()\n",
    "            if negative_prices > 0:\n",
    "                violations.append(f\"Rule 4 Violation: {negative_prices} records with non-positive {col}\")\n",
    "                print(f\"  Rule 4: Found {negative_prices} records with invalid {col}\")\n",
    "            else:\n",
    "                print(f\"  Rule 4: All {col} values are positive - PASS\")\n",
    "    \n",
    "    # Rule 5: Extreme price variations (potential data entry errors)\n",
    "    if 'Unit_Price' in df_validated.columns and 'Negotiated_Price' in df_validated.columns:\n",
    "        # Flag records where negotiated price is more than 200% of unit price\n",
    "        extreme_increases = (df_validated['Negotiated_Price'] > df_validated['Unit_Price'] * 2).sum()\n",
    "        # Flag records where negotiated price is less than 10% of unit price\n",
    "        extreme_decreases = (df_validated['Negotiated_Price'] < df_validated['Unit_Price'] * 0.1).sum()\n",
    "        \n",
    "        if extreme_increases > 0:\n",
    "            violations.append(f\"Rule 5 Violation: {extreme_increases} records with extreme price increases (>200%)\")\n",
    "            print(f\"  Rule 5a: Found {extreme_increases} records with extreme price increases\")\n",
    "        \n",
    "        if extreme_decreases > 0:\n",
    "            violations.append(f\"Rule 5 Violation: {extreme_decreases} records with extreme price decreases (<10%)\")\n",
    "            print(f\"  Rule 5b: Found {extreme_decreases} records with extreme price decreases\")\n",
    "        \n",
    "        if extreme_increases == 0 and extreme_decreases == 0:\n",
    "            print(\"  Rule 5: No extreme price variations detected - PASS\")\n",
    "    \n",
    "    return df_validated, violations\n",
    "\n",
    "# Run business rule validation\n",
    "df_validated, rule_violations = validate_business_rules(df_cleaned)\n",
    "\n",
    "print(f\"\\nValidation Summary:\")\n",
    "print(f\"Total rule violations: {len(rule_violations)}\")\n",
    "if rule_violations:\n",
    "    print(\"\\nViolations detected:\")\n",
    "    for violation in rule_violations:\n",
    "        print(f\"  - {violation}\")\nelse:\n",
    "    print(\"All business rules passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Outlier Detection and Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers in numerical columns\n",
    "def detect_outliers(df: pd.DataFrame, columns: List[str] = None, method: str = 'iqr') -> Dict[str, List[int]]:\n",
    "    \"\"\"\n",
    "    Detect outliers using IQR or Z-score method.\n",
    "    \"\"\"\n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    outliers = {}\n",
    "    \n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            if method == 'iqr':\n",
    "                Q1 = df[col].quantile(0.25)\n",
    "                Q3 = df[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                \n",
    "                outlier_mask = (df[col] < lower_bound) | (df[col] > upper_bound)\n",
    "                outliers[col] = df[outlier_mask].index.tolist()\n",
    "            \n",
    "            elif method == 'zscore' and SCIPY_AVAILABLE:\n",
    "                z_scores = np.abs(stats.zscore(df[col].dropna()))\n",
    "                outlier_mask = z_scores > 3\n",
    "                outliers[col] = df[col].dropna()[outlier_mask].index.tolist()\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "# Detect outliers\n",
    "print(\"Outlier Detection:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Focus on key numerical columns\n",
    "outlier_columns = ['Quantity', 'Unit_Price', 'Negotiated_Price', 'Defective_Units']\n",
    "outliers_detected = detect_outliers(df_validated, outlier_columns, method='iqr')\n",
    "\n",
    "for col, outlier_indices in outliers_detected.items():\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Outliers detected: {len(outlier_indices)}\")\n",
    "    \n",
    "    if len(outlier_indices) > 0:\n",
    "        outlier_values = df_validated.loc[outlier_indices, col]\n",
    "        print(f\"  Range: {outlier_values.min():.2f} to {outlier_values.max():.2f}\")\n",
    "        print(f\"  Percentage of data: {len(outlier_indices)/len(df_validated)*100:.2f}%\")\n",
    "        \n",
    "        # Show some examples\n",
    "        if len(outlier_indices) <= 5:\n",
    "            print(f\"  Values: {outlier_values.tolist()}\")\n",
    "        else:\n",
    "            print(f\"  Sample values: {outlier_values.head().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outliers\n",
    "numerical_cols = ['Quantity', 'Unit_Price', 'Negotiated_Price', 'Defective_Units']\n",
    "available_cols = [col for col in numerical_cols if col in df_validated.columns]\n",
    "\n",
    "if available_cols:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, col in enumerate(available_cols[:4]):\n",
    "        if i < len(axes):\n",
    "            # Box plot\n",
    "            df_validated[col].plot(kind='box', ax=axes[i])\n",
    "            axes[i].set_title(f'{col} - Box Plot')\n",
    "            axes[i].set_ylabel('Value')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for j in range(len(available_cols), len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create outlier summary\n",
    "total_outliers = sum(len(indices) for indices in outliers_detected.values())\n",
    "print(f\"\\nOutlier Summary:\")\n",
    "print(f\"Total outlier instances: {total_outliers}\")\n",
    "print(f\"Percentage of dataset: {total_outliers/(len(df_validated)*len(available_cols))*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final clean dataset\n",
    "def create_clean_dataset(df: pd.DataFrame, handle_outliers: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create final clean dataset with optional outlier handling.\n",
    "    \"\"\"\n",
    "    df_final = df.copy()\n",
    "    \n",
    "    # Add data quality flags\n",
    "    df_final['data_quality_score'] = 100  # Start with perfect score\n",
    "    \n",
    "    # Reduce score for various issues\n",
    "    if 'excess_defects_flag' in df_final.columns:\n",
    "        df_final.loc[df_final['excess_defects_flag'], 'data_quality_score'] -= 20\n",
    "    \n",
    "    # Add derived fields that might be useful\n",
    "    if 'Order_Date' in df_final.columns and 'Delivery_Date' in df_final.columns:\n",
    "        # Calculate lead time\n",
    "        df_final['lead_time_days'] = (df_final['Delivery_Date'] - df_final['Order_Date']).dt.days\n",
    "    \n",
    "    if 'Unit_Price' in df_final.columns and 'Negotiated_Price' in df_final.columns and 'Quantity' in df_final.columns:\n",
    "        # Calculate financial metrics\n",
    "        df_final['total_original_value'] = df_final['Unit_Price'] * df_final['Quantity']\n",
    "        df_final['total_negotiated_value'] = df_final['Negotiated_Price'] * df_final['Quantity']\n",
    "        df_final['cost_savings'] = df_final['total_original_value'] - df_final['total_negotiated_value']\n",
    "        df_final['savings_percentage'] = (df_final['cost_savings'] / df_final['total_original_value'] * 100).round(2)\n",
    "    \n",
    "    if 'Defective_Units' in df_final.columns and 'Quantity' in df_final.columns:\n",
    "        # Calculate quality metrics\n",
    "        df_final['defect_rate'] = (df_final['Defective_Units'] / df_final['Quantity'] * 100).round(2)\n",
    "        df_final['quality_score'] = 100 - df_final['defect_rate']\n",
    "    \n",
    "    # Handle extreme outliers if requested\n",
    "    if handle_outliers:\n",
    "        # Cap extreme values rather than removing them\n",
    "        numerical_cols = ['Quantity', 'Unit_Price', 'Negotiated_Price']\n",
    "        for col in numerical_cols:\n",
    "            if col in df_final.columns:\n",
    "                # Cap at 99th percentile\n",
    "                cap_value = df_final[col].quantile(0.99)\n",
    "                outlier_count = (df_final[col] > cap_value).sum()\n",
    "                if outlier_count > 0:\n",
    "                    df_final[col] = df_final[col].clip(upper=cap_value)\n",
    "                    print(f\"Capped {outlier_count} extreme values in {col} at {cap_value:.2f}\")\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "print(\"Creating Final Clean Dataset:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create clean dataset without outlier handling (preserve original data)\n",
    "df_clean_final = create_clean_dataset(df_validated, handle_outliers=False)\n",
    "\n",
    "print(f\"Final dataset shape: {df_clean_final.shape}\")\n",
    "print(f\"New columns added: {set(df_clean_final.columns) - set(df_raw.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final data quality summary\n",
    "print(\"Final Data Quality Summary:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Original vs cleaned comparison\n",
    "print(\"Dataset Comparison:\")\n",
    "print(f\"  Original records: {len(df_raw):,}\")\n",
    "print(f\"  Final records: {len(df_clean_final):,}\")\n",
    "print(f\"  Records retained: {len(df_clean_final)/len(df_raw)*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n  Original columns: {len(df_raw.columns)}\")\n",
    "print(f\"  Final columns: {len(df_clean_final.columns)}\")\n",
    "print(f\"  New columns added: {len(df_clean_final.columns) - len(df_raw.columns)}\")\n",
    "\n",
    "# Missing values summary\n",
    "original_missing = df_raw.isnull().sum().sum()\n",
    "final_missing = df_clean_final.isnull().sum().sum()\n",
    "print(f\"\\nMissing Values:\")\n",
    "print(f\"  Original missing: {original_missing:,}\")\n",
    "print(f\"  Final missing: {final_missing:,}\")\n",
    "print(f\"  Missing values handled: {original_missing - final_missing:,}\")\n",
    "\n",
    "# Data quality score summary\n",
    "if 'data_quality_score' in df_clean_final.columns:\n",
    "    avg_quality_score = df_clean_final['data_quality_score'].mean()\n",
    "    high_quality_records = (df_clean_final['data_quality_score'] >= 90).sum()\n",
    "    print(f\"\\nData Quality Metrics:\")\n",
    "    print(f\"  Average quality score: {avg_quality_score:.1f}/100\")\n",
    "    print(f\"  High quality records (>=90): {high_quality_records:,} ({high_quality_records/len(df_clean_final)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export cleaned datasets\n",
    "import os\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "os.makedirs('../data/interim', exist_ok=True)\n",
    "\n",
    "# Export main cleaned dataset\n",
    "output_path = '../data/processed/procurement_data_clean.csv'\n",
    "df_clean_final.to_csv(output_path, index=False)\n",
    "print(f\"Clean dataset exported to: {output_path}\")\n",
    "\n",
    "# Export data quality report\n",
    "quality_report_path = '../data/processed/data_quality_report.csv'\n",
    "final_quality_report = assess_data_quality(df_clean_final)\n",
    "final_quality_report.to_csv(quality_report_path, index=False)\n",
    "print(f\"Quality report exported to: {quality_report_path}\")\n",
    "\n",
    "# Export cleaning summary\n",
    "summary_path = '../data/processed/cleaning_summary.txt'\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(\"PROCUREMENT DATA CLEANING SUMMARY\\n\")\n",
    "    f.write(\"=\" * 50 + \"\\n\")\n",
    "    f.write(f\"Cleaning Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Original Dataset: {df_raw.shape[0]:,} rows x {df_raw.shape[1]} columns\\n\")\n",
    "    f.write(f\"Final Dataset: {df_clean_final.shape[0]:,} rows x {df_clean_final.shape[1]} columns\\n\")\n",
    "    f.write(f\"Records Retained: {len(df_clean_final)/len(df_raw)*100:.2f}%\\n\")\n",
    "    f.write(f\"Missing Values Handled: {original_missing - final_missing:,}\\n\")\n",
    "    f.write(f\"Business Rule Violations: {len(rule_violations)}\\n\")\n",
    "    \n",
    "    if rule_violations:\n",
    "        f.write(\"\\nRule Violations:\\n\")\n",
    "        for violation in rule_violations:\n",
    "            f.write(f\"  - {violation}\\n\")\n",
    "    \n",
    "    f.write(f\"\\nNew Columns Added:\\n\")\n",
    "    new_columns = set(df_clean_final.columns) - set(df_raw.columns)\n",
    "    for col in sorted(new_columns):\n",
    "        f.write(f\"  - {col}\\n\")\n",
    "\n",
    "print(f\"Cleaning summary exported to: {summary_path}\")\n",
    "\n",
    "# Display final dataset sample\n",
    "print(f\"\\nFinal Clean Dataset Sample:\")\n",
    "print(\"=\" * 50)\n",
    "display(df_clean_final.head())\n",
    "\n",
    "print(f\"\\nData cleaning completed successfully!\")\n",
    "print(f\"Files generated:\")\n",
    "print(f\"  1. {output_path}\")\n",
    "print(f\"  2. {quality_report_path}\")\n",
    "print(f\"  3. {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Cleaning Complete!\n",
    "\n",
    "**Accomplishments:**\n",
    "- Comprehensive data quality assessment\n",
    "- Missing value handling with business logic\n",
    "- Categorical data standardization\n",
    "- Date format standardization\n",
    "- Business rule validation\n",
    "- Outlier detection and analysis\n",
    "- Quality score assignment\n",
    "- Derived field creation\n",
    "\n",
    "**Key Outputs:**\n",
    "- `../data/processed/procurement_data_clean.csv` - Main clean dataset\n",
    "- `../data/processed/data_quality_report.csv` - Quality metrics\n",
    "- `../data/processed/cleaning_summary.txt` - Detailed summary\n",
    "\n",
    "**Next Steps:**\n",
    "1. **Feature Engineering** (Notebook 03) - Create advanced analytical features\n",
    "2. **KPI Analysis** (Notebook 04) - Calculate and analyze key performance indicators\n",
    "3. **Supplier Performance** (Notebook 05) - Deep dive into supplier metrics\n",
    "\n",
    "**Data Quality Notes:**\n",
    "- Review any business rule violations flagged above\n",
    "- Consider domain expert input for extreme outliers\n",
    "- Validate derived calculations with business stakeholders\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}